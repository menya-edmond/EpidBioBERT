# -*- coding: utf-8 -*-
"""Doc_SpacioTemp_Epid_FT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xDYBzczKdAEgYXDcZxcyRYtoFqQ9mbA0

!pip install pytorch-pretrained-bert
!pip install transformers
"""

import os
import re
import csv
import itertools
import json

import nltk
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm import tqdm, trange
from collections import defaultdict, OrderedDict

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from torch.utils.data import RandomSampler, SequentialSampler
from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig
from transformers import BertForTokenClassification, AdamW
from transformers import get_linear_schedule_with_warmup
from sklearn.metrics import f1_score,precision_score,recall_score
from sklearn.preprocessing import LabelEncoder
from matplotlib.ticker import AutoMinorLocator

import tensorflow as tf
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from nltk import FreqDist

import pytorch_pretrained_bert

device_name = tf.test.gpu_device_name()

if device_name == '/device:GPU:0':
  print('Found GPU at: {}'.format(device_name))
else:
  raise SystemError('GPU device not found')


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



MAX_LEN=200
BATCH_SIZE=16
#BATCH_SIZE=1
tokenizer = BertTokenizer(vocab_file='biobert_v1.1_pubmed/vocab.txt', do_lower_case=False)

def tags_list_computer(candidates_list,raw_text_list,thematic_feature):
  """Labels thematic features from there JSON data annotations
  
  
  """
  
  del_features=[]

  for candidate in candidates_list:
    feature_text=[]
    feature_type=re.findall(re.compile('type=\"(.*?)\"'),candidate)[0]
    length=int(re.findall(re.compile('length=\"(.*?)\"'),candidate)[0])
    index=int(re.findall(re.compile('index=(.*?) '),candidate)[0])
    candidate_no=int(re.findall(re.compile('id=\"c(.*?)\"'),candidate)[0])
    
    if feature_type == thematic_feature:
      for i in range(index,index+length):
        del_features.append(raw_text_list[i])  
  return [feat for feat in raw_text_list if feat not in del_features]

def feature_del(data,thematic_feature):
  """Finds all candidate thematic features by tag from news article
  
  """
  
  raw_text_list=re.findall(re.compile('<token id=\"t\d+\"> (.*?) </token>'),data['tagged_content'])
  candidates_list=re.findall(re.compile('<candidate (.*?)></candidate> '),data['tagged_content'])  
  re_raw_list=tags_list_computer(candidates_list,raw_text_list,thematic_feature)
  raw_doc = ' '.join([str(elem) for elem in re_raw_list])  
  return raw_doc

corpora = '/content/gdrive/MyDrive/Colab Data/articles'
tagged_data=[]
irrelevant_docs=0
irrelevant_docs_address=[]
df_corpus={'Content':[],'Label':[]}
del_thematic_feature='none' #thematic feature to be ignored either date, disease, location or host

for subdir, dirs, files in os.walk(corpora):
  """Walk through the directory containing PADI-Web JSON News datasets
    Clean the corpus and select relevant and irrelevant documents
    
  """
  for file in files:
    path = os.path.join(subdir, file)
    
    with open(path,'r') as f:
      data=json.load(f)
      if data['candidateLabels']: #not an empty list thus relevant document

        if data['content']:
          content=feature_del(data,del_thematic_feature)
          df_corpus['Content'].append(content.replace('\r', '').replace('\t', '').replace('\n','').replace('<br /> ',''))
          df_corpus['Label'].append('relevant')

      else:#irrelevant document
        if data['content']:
          content=feature_del(data,del_thematic_feature)
          df_corpus['Content'].append(content.replace('\r', '').replace('\t', '').replace('\n','').replace('<br /> ',''))
          df_corpus['Label'].append('irrelevant')

data=pd.DataFrame(df_corpus)

data.head()

label_encoder=LabelEncoder()
label_encoder.fit(data['Label'])

X=list(data['Content'])
y=list(label_encoder.transform(data['Label']))

def tok_with_labels(doc, label):
  """tokenize and keep labels intact"""
  
  tok_doc=tokenizer.tokenize(doc)
  
  return tok_doc, [label]

tok_texts_and_labels = [tok_with_labels(sent, labs) for sent, labs in zip(X, y)]

tok_texts = [tok_label_pair[0] for tok_label_pair in tok_texts_and_labels]
labels = [tok_label_pair[1] for tok_label_pair in tok_texts_and_labels]

input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tok_texts],
                          maxlen=MAX_LEN, dtype="long", value=0.0,
                          truncating="post", padding="post")

"""for char in tok_texts:
    print('WordPiece Tokenizer Preview:\n', char)
    break"""

# attention masks make explicit reference to which tokens are actual words vs padded words
attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]

split_size=.2

tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, labels,
                                                            random_state=2018, test_size=split_size)
tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,
                                             random_state=2018, test_size=split_size)

n_attention_masks = [[float(i != 0.0) for i in ii] for ii in tr_inputs]

tr_masks, test_masks, _, _ = train_test_split(n_attention_masks, tr_inputs,
                                             random_state=2018, test_size=split_size)

tr_inputs, test_inputs, tr_tags, test_tags = train_test_split(tr_inputs, tr_tags,
                                                            random_state=2018, test_size=split_size)

tr_inputs = torch.tensor(tr_inputs)
val_inputs = torch.tensor(val_inputs)
test_inputs = torch.tensor(test_inputs)


tr_tags = torch.tensor(tr_tags)
val_tags = torch.tensor(val_tags)
test_tags = torch.tensor(test_tags)

tr_masks = torch.tensor(tr_masks)
val_masks = torch.tensor(val_masks)
test_masks = torch.tensor(test_masks)

tr_tags.shape

train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE, drop_last=True)

valid_data = TensorDataset(val_inputs, val_masks, val_tags)
valid_sampler = SequentialSampler(valid_data)
valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE, drop_last=True)

test_data = TensorDataset(test_inputs, test_masks, test_tags)
test_sampler = SequentialSampler(test_data)
test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE, drop_last=True)


config = BertConfig.from_json_file('biobert_v1.1_pubmed/config.json')
tmp_d = torch.load('biobert_v1.1_pubmed/pytorch_model.bin', map_location=device)
state_dict = OrderedDict()

for i in list(tmp_d.keys())[:199]:
    x = i
    if i.find('bert') > -1:
        x = '.'.join(i.split('.')[1:])
    state_dict[x] = tmp_d[i]

state_dict['embeddings.word_embeddings.weight'].shape

vocab_len=2

class BioBert(nn.Module):

  def __init__(self, vocab_len, config, state_dict):
    super().__init__()
    self.bert = BertModel(config)
    self.bert.load_state_dict(state_dict, strict=False)
    self.dropout = nn.Dropout(p=0.3)
    self.batchnorm=nn.BatchNorm1d(1,affine=False)
    self.output = nn.Linear(self.bert.config.hidden_size, vocab_len)
    self.softmax = nn.Softmax(dim=1)

  def forward(self, input_ids, attention_mask):
    encoded_layer, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)
    encl = encoded_layer[-1]
    max_pooled, _  = torch.max(encl, 1)
    norm=torch.squeeze(self.batchnorm(torch.unsqueeze(max_pooled,1)),0)
    dropped = self.dropout(norm)
    out = self.output(dropped)
    return out, out.argmax(-1)

model = BioBert(vocab_len,config,state_dict)
model.to(device)

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate': 0.0}
    ]

optimizer = AdamW(
    optimizer_grouped_parameters,
    lr=1e-5,
    eps=1e-8
)
epochs = 50
max_grad_norm = 1.0

total_steps = len(train_dataloader.dataset) * epochs
warmup_steps = int(epochs * len(train_dataloader.dataset) * 0.1 / BATCH_SIZE) #mine


scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    #num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)
loss_fn = nn.CrossEntropyLoss().to(device)

def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler):
    model = model.train()
    losses = []
    correct_predictions = 0.
    batch_no = 1
    for step,batch in enumerate(data_loader):

        batch = tuple(t.to(device) for t in batch)
        b_input_ids, b_input_mask, b_labels = batch
        
        outputs,y_hat = model(b_input_ids,b_input_mask)
        _,preds = torch.max(outputs,dim=2)

        outputs = outputs.view(-1,outputs.shape[-1])
        b_labels_shaped = b_labels.view(-1)

        loss=loss_fn(outputs,b_labels_shaped)
        correct_predictions += torch.sum(preds == b_labels)
        losses.append(loss.item())
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
        
        batch_no=batch_no+1

    return correct_predictions.double()/len(data_loader.dataset) , np.mean(losses)

def model_eval(model,data_loader,loss_fn,device):
    model = model.eval()
    
    losses = []
    correct_predictions = 0.

    original_labels, new_labels = [], []
    
    with torch.no_grad():
        for step, batch in enumerate(data_loader):
            batch = tuple(t.to(device) for t in batch)
            b_input_ids, b_input_mask, b_labels = batch
            
        
            outputs,y_hat = model(b_input_ids,b_input_mask)
        
            _,preds = torch.max(outputs,dim=2)

            outputs = outputs.view(-1,outputs.shape[-1])
            b_labels_shaped = b_labels.view(-1)
            loss = loss_fn(outputs,b_labels_shaped)
            correct_predictions += torch.sum(preds == b_labels)
            losses.append(loss.item())

            label_indices = y_hat.to('cpu').numpy()
            pred_indices = b_labels_shaped.to('cpu').numpy()

            for label_idx,pred_idx in zip(label_indices,pred_indices):
              new_labels.append(label_idx[0])
              original_labels.append(pred_idx)

        print('\nPrecision_Score: ',precision_score(original_labels,new_labels,pos_label=1))
        print('\nRecall_Score: ',recall_score(original_labels,new_labels,pos_label=1))
        print('\nF_Score: ',f1_score(original_labels,new_labels,pos_label=1))
    return correct_predictions/len(data_loader.dataset) , np.mean(losses)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = defaultdict(list)
# loss_values = []
# 
# for epoch in range(epochs):
#     
#     total_loss = 0
#     print(f'======== Epoch {epoch+1}/{epochs} ========')
#     train_acc,train_loss = train_epoch(model,train_dataloader,loss_fn,optimizer,device,scheduler)
#     print(f'Train Loss: {train_loss} Train Accuracy: {train_acc}')
#     total_loss += train_loss.item()
#     
#     avg_train_loss = total_loss / len(train_dataloader.dataset)  
#     loss_values.append(avg_train_loss)
#     
#     val_acc,val_loss = model_eval(model,valid_dataloader,loss_fn,device)
#     print(f'Val Loss: {val_loss} Val Accuracy: {val_acc}')
#     
#     history['train_loss'].append(train_loss)
#     history['train_acc'].append(train_acc)
#     
#     history['val_loss'].append(val_loss)
#     history['val_acc'].append(val_acc)

sns.set(style='darkgrid')

sns.set(font_scale=1.5)
plt.rcParams["figure.figsize"] = (12,6)

# learning curve
plt.plot(loss_values, 'b-o')
plt.plot(history['train_loss'],'g--',label='Train Loss')
plt.plot(history['val_loss'],'b',label='Val Loss')

plt.title("Training loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.show()

our_loss=history['train_loss']


sns.set_style('ticks',{'ytick.direction': 'in','xtick.direction': 'in','xtick.top': True,'ytick.right': True,'axes.spines.left': True})

fig, ax = plt.subplots()
# learning curve

ax.plot(history['train_loss'],'b',label='EpidBioBERT')


minor_locator = AutoMinorLocator(2)

ax.tick_params(axis='x', which='minor', bottom=False, top=False)
ax.yaxis.set_minor_locator(minor_locator)
ax.yaxis.grid(True,which='major',linestyle='--',color='grey') 
ax.tick_params(axis='x', which='minor', bottom=False)


plt.title("Training loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.minorticks_on()
plt.legend()

plt.show()
